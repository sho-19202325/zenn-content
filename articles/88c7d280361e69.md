---
title: "スッキリわかるPython機械学習入門 第7章 分類2: 客船沈没事故での生存予測"
emoji: "😊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

## この章でやること

乗客の特徴から生存か死亡かに分類するモデルを作成する。
また、その過程でどのような特徴を持つ人が生き残れたかを考察する。

## データの前処理

### 不均衡データの調査

正解データの件数の比率に差があるようなものを不均衡データと呼ぶ。

今回の場合
- 生存: 549件
- 死亡: 342件

となり、生存者のデータは死亡者のデータの1.6倍になっている。

### 不均衡データが存在するとなぜ問題か

例えば
- 生存: 5% 
- 死亡: 95%

のような比率でデータが偏っている場合。

- 条件: Age >= 0
- YES: 死亡
- NO: 生存

という決定木を作ると、
全てのデータが該当するにもかかわらず、死亡の割合が95%になってしまう。
※不純度が5%になる

そのため、このモデルは全てのデータに対して死亡と判断してしまう。

### 外れ値の取り出しについて

重回帰分析では一つの外れ値が影響を与えるため、散布図を用いて外れ値を特定する必要があった。
一方決定木では、一件ぐらい外れ値があってもモデルに影響は与えないため、外れ値を取り出す必要がない。

※ 決定木が外れ値の影響を受けにくいだけで、分類モデルの中にも外れ値の影響を受けるものもある。

## モデルの作成と学習

### 不均衡データの考慮

```DecisionTreeClassifier```の引数に```class_weight = 'balanced'```を追加する。

```python
model = tree.DecisionTreeClassifier(max_depth=5, random_state=0, class_weight='balanced')
model.fit(x_train, y_train)
```

## モデルの評価

### 過学習とは

過学習とは教師あり学習において、複雑なモデルで学習すればするほど、
学習に利用した訓練データでの予測性能は上がるが、テストデータでの予測性能は低くなるという現象のこと。

### なぜ過学習が起こるのか

訓練データのとても細かい特徴までも法則として学習してしまう。
つまり、訓練データ専用モデルが出来上がるということ。

過学習は決定木分析だけではなく教師あり学習の全てに起こりうる。
決定木分析では木の深さを増やしすぎると過学習を起こしやすくなり、
重回帰分析では、特徴量の列の数を増やしすぎると過学習が起こりやすい。

### ピポットテーブルによる穴埋め

これまでは欠損値の行を除いた全データでAge列の平均値を計算し、その値で穴埋めを行なっていた。
しかし、特定の特徴量に切り分けた小グループごとにまとめるとAgeの分布が大きく異なる場合がある。

例) 性別ごとの年齢分布など

※ この時の「性別」のような、小グループごとに分ける際の基準となる列を基準軸と呼ぶ。

基準軸は複数選ぶこともできる。
複数選ぶ場合の集計を**クロス集計**といい、pandasのピポットテーブル

### ダミー変数化

性別など、文字列の入ったデータも特徴量として用いることができる。
例えば男女別の生存率を比較する。

※ ```# In[19]:```を参照

すると女性の生存率の方が圧倒的に高いことがわかる。
しかしこれを直接特徴量として学習(model.fit)させるとエラーが出る。

=> scikit-learnでは特徴量には数値の列しか追加できないため。

これを学習可能な状態にする場合、性別の場合は、
- 女性 => 0
- 男性 => 1

生存の場合は、
- No => 0
- Yes => 1

のように文字列を数値に置き換えてやる。
これをダミー変数化(ワンホットコーディング)という。

### 文字列が3つ以上ある場合は?

搭乗港の場合'C', 'Q', 'S'の三つ文字列がある。
このような場合ダミー変数は使える?
=> 使えない

本書の説明:

- C => 0
- Q => 1
- S => 2


と変換した場合、モデルの学習時には内部で1+1 = 2という計算が行われた場合、
'Q'+'Q' = 'S'となってしまう。
なんかあんまりハマってない？

独自の説明:

今回の文字列の場合、各文字ごとが独立しており、'C'と'S'の間が'Q'にはならないし、数字の並び方に意味がない。
この場合C, C, S,Sのデータがあるとき、平均は1だから'Q'になってしまうが、これは明確に誤り。

ただ一方でアンケート結果などを考える場合
- よく当てはまる => 0
- まあまあ当てはまる => 1
- どちらでもない => 2
- あまり当てはまらない => 3
- 当てはまらない => 4

のように数字が小さくなるほど、当てはまることを意味する。
このような場合はダミー変数かが有効。


### 決定木における特徴量の考察

重回帰分析では、特徴量にかかる重みの大きさが、影響力の大きさを意味していた。
決定木も場合もこのような解釈をする方法がある。

※決定木なので、他のモデルはまた違った解釈方がある、はず。

決定技の場合、より上にある分岐条件ほど「正解データの分類に影響を与える」。
例えば条件1で'Yes'の時、条件2は見られない。
※図7-10参照

図を見て判断しても良いが、scikit-learnでは**特徴量重要度**という指標がある。
特徴量重要度は0-1の値をとり、値が大きいほど**その列が分類に与える影響が大きい**ことを意味する。
